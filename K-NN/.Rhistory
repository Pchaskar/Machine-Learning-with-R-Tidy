library(kknn)
# Step 4: Define the Model
knn_spec <- nearest_neighbor(neighbors = 21) %>%
set_engine("kknn") %>%
set_mode("classification")
# Step 5: Train the Model
knn_fit <- knn_spec %>%
fit(diagnosis ~ ., data = train_preprocessed)
test_predictions <- predict(knn_fit, new_data = test_preprocessed) %>%
bind_cols(test_preprocessed %>% select(diagnosis))
View(test_predictions)
# Metrics
metrics <- test_predictions %>%
metrics(truth = diagnosis, estimate = .pred_class)
# Confusion Matrix
conf_matrix <- test_predictions %>%
conf_mat(truth = diagnosis, estimate = .pred_class)
print(metrics)
print(conf_matrix)
knn_workflow <- workflow() %>%
add_model(nearest_neighbor(neighbors = tune()) %>%
set_engine("kknn") %>%
set_mode("classification")) %>%
add_recipe(rec)
# Cross-validation
set.seed(123)
cv_folds <- vfold_cv(train_data, v = 5, strata = diagnosis)
cv_folds
# Tune K
knn_grid <- grid_regular(neighbors(range = c(1, 31)), levels = 15)
knn_workflow <- workflow() %>%
add_model(nearest_neighbor(neighbors = tune()) %>%
set_engine("kknn") %>%
set_mode("classification")) %>%
add_recipe(rec)
rec <- recipe(diagnosis ~ ., data = train_data) %>%
step_normalize(all_predictors())
knn_workflow <- workflow() %>%
add_model(nearest_neighbor(neighbors = tune()) %>%
set_engine("kknn") %>%
set_mode("classification")) %>%
add_recipe(rec)
# Cross-validation
set.seed(123)
cv_folds <- vfold_cv(train_data, v = 5, strata = diagnosis)
# Tune K
knn_grid <- grid_regular(neighbors(range = c(1, 31)), levels = 15)
knn_results <- tune_grid(
knn_workflow,
resamples = cv_folds,
grid = knn_grid,
metrics = metric_set(accuracy)
)
best_k <- knn_results %>%
select_best("accuracy")
# Best K
best_k <- knn_results %>%
select_best(metric = "accuracy")
print(best_k)
# Final Model
final_knn <- finalize_workflow(knn_workflow, best_k)
# Fit on full training set and evaluate on test set
final_fit <- final_knn %>%
last_fit(data_split)
# Collect metrics from the final model
collect_metrics(final_fit)
# Step 6: Evaluate the Model
# Make predictions on test data
test_predictions <- predict(final_fit, new_data = test_preprocessed) %>%
bind_cols(test_preprocessed %>% select(diagnosis))
# Make predictions on the test data
test_predictions <- predict(final_knn_fit, new_data = test_data)
# Make predictions on the test data
test_predictions <- predict(final_fit, new_data = test_data)
##### Chapter 3: Classification using Nearest Neighbors --------------------
# Load required libraries
library(tidymodels)  # For data splitting, preprocessing, modeling, etc.
library(kknn)        # For K-Nearest Neighbors model
# ---------------------------
# Step 1: Load and Prepare Data
# ---------------------------
# Import the dataset
wbcd <- read.csv("wisc_bc_data.csv", stringsAsFactors = FALSE)
# Remove unnecessary columns (e.g., 'id' column)
wbcd <- wbcd %>%
select(-id)
# Recode the 'diagnosis' column as a factor (Benign, Malignant)
wbcd <- wbcd %>%
mutate(diagnosis = factor(diagnosis, levels = c("B", "M"),
labels = c("Benign", "Malignant")))
# Set seed for reproducibility
set.seed(123)
# Split the data into training (80%) and testing (20%) sets, stratified by 'diagnosis'
data_split <- initial_split(wbcd, prop = 0.8, strata = diagnosis)
# Assign training and testing datasets
train_data <- training(data_split)
test_data <- testing(data_split)
# Use a recipe for consistent preprocessing steps
# Step 3.1: Normalize all predictors (features) for the training and testing sets
rec <- recipe(diagnosis ~ ., data = train_data) %>%
step_normalize(all_predictors()) %>%  # Normalize all predictors (numeric features)
prep()  # Preprocessing on the training data
# Apply preprocessing (normalization) to both training and test data
train_preprocessed <- bake(rec, new_data = train_data)
test_preprocessed <- bake(rec, new_data = test_data)
knn_workflow <- workflow() %>%
add_recipe(rec) %>%  # Add the preprocessing recipe
add_model(nearest_neighbor(neighbors = tune()) %>%  # Tune the number of neighbors (k)
set_engine("kknn") %>%
set_mode("classification"))
##### Chapter 3: Classification using Nearest Neighbors --------------------
# Load required libraries
library(tidymodels)  # For data splitting, preprocessing, modeling, etc.
library(kknn)        # For K-Nearest Neighbors model
# ---------------------------
# Step 1: Load and Prepare Data
# ---------------------------
# Import the dataset
wbcd <- read.csv("wisc_bc_data.csv", stringsAsFactors = FALSE)
# Remove unnecessary columns (e.g., 'id' column)
wbcd <- wbcd %>%
select(-id)
# Recode the 'diagnosis' column as a factor (Benign, Malignant)
wbcd <- wbcd %>%
mutate(diagnosis = factor(diagnosis, levels = c("B", "M"),
labels = c("Benign", "Malignant")))
# ---------------------------
# Step 2: Split the Data
# ---------------------------
# Set seed for reproducibility
set.seed(123)
# Split the data into training (80%) and testing (20%) sets, stratified by 'diagnosis'
data_split <- initial_split(wbcd, prop = 0.8, strata = diagnosis)
# Assign training and testing datasets
train_data <- training(data_split)
test_data <- testing(data_split)
# ---------------------------
# Step 3: Preprocessing (Unprocessed Recipe)
# ---------------------------
# Use a recipe for consistent preprocessing steps
# Do not apply prep() yet, leave it for the workflow
rec <- recipe(diagnosis ~ ., data = train_data) %>%
step_normalize(all_predictors())  # Normalize all predictors (numeric features)
# ---
# Create a workflow for hyperparameter tuning
knn_workflow <- workflow() %>%
add_recipe(rec) %>%  # Add the unprocessed recipe
add_model(nearest_neighbor(neighbors = tune()) %>%  # Tune the number of neighbors (k)
set_engine("kknn") %>%
set_mode("classification"))
# Perform 5-fold cross-validation on the training data, stratified by 'diagnosis'
set.seed(123)
cv_folds <- vfold_cv(train_data, v = 5, strata = diagnosis)
# Define a grid of values for the number of neighbors (k) to search over
knn_grid <- grid_regular(neighbors(range = c(1, 31)), levels = 15)
# Perform hyperparameter tuning by evaluating different values of k using cross-validation
knn_results <- tune_grid(
knn_workflow,
resamples = cv_folds,
grid = knn_grid,
metrics = metric_set(accuracy)  # Optimize for accuracy
)
# -------
# Extract the best value of k based on accuracy
best_k <- knn_results %>%
select_best(metric = "accuracy")
# Print the best k (number of neighbors)
print(best_k)
final_knn <- finalize_workflow(knn_workflow, best_k)
# Fit the final model on the entire training set
final_fit <- final_knn %>%
last_fit(data_split)
final_metrics <- collect_metrics(final_fit)
# Print final model evaluation metrics
print(final_metrics)
# Make predictions on the test data using the final fitted model
test_predictions <- predict(final_fit, new_data = test_data)
# Make predictions on the test data using the final fitted model
test_predictions <- predict(final_fit, new_data = test_data)
# Step 8: Finalize the Model
# ---------------------------
# Finalize the workflow by incorporating the best k value
final_knn <- finalize_workflow(knn_workflow, best_k)
# Fit the final model on the entire training set
final_fit <- final_knn %>%
last_fit(data_split)
# Collect performance metrics from the final model
final_metrics <- collect_metrics(final_fit)
# Print final model evaluation metrics
print(final_metrics)
# Step 10: Make Predictions with Final Model
# ---------------------------
# Extract the fitted model and the preprocessed test data from the last_fit() result
final_model <- extract_workflow(final_fit)  # Extract the final workflow object
# Make predictions on the test data using the final fitted model
test_predictions <- predict(final_model, new_data = test_data)
# Optionally, view the predictions along with the actual labels
predictions_with_labels <- test_predictions %>%
bind_cols(test_data %>% select(diagnosis))  # Assuming 'diagnosis' is the true label
# View the first few predictions
head(predictions_with_labels)
# ---------------------------
# Confusion Matrix to assess the classification performance
conf_matrix <- test_predictions %>%
conf_mat(truth = diagnosis, estimate = .pred_class)
conf_matrix <- predictions_with_labels %>%
conf_mat(truth = diagnosis, estimate = .pred_class)
# Print the confusion matrix
print(conf_matrix)
library(MLmetrics) # For MCC
library(pROC)  # For AUC calculation and ROC plotting
library(ROCR)  # For ROC curve
library(kknn)
library(tidymodels)
library(kknn)
library(ROCR)  # For ROC curve
library(pROC)  # For AUC calculation and ROC plotting
library(ggplot2) # For visualization
library(MLmetrics) # For MCC
# Step 1: Load and Prepare Data
wbcd <- read.csv("wisc_bc_data.csv", stringsAsFactors = FALSE)
# Remove unnecessary columns (e.g., ID)
wbcd <- wbcd %>%
select(-id)
# Recode the 'diagnosis' column as a factor
wbcd <- wbcd %>%
mutate(diagnosis = factor(diagnosis, levels = c("B", "M"),
labels = c("Benign", "Malignant")))
# Step 2: Split the Data
set.seed(123) # For reproducibility
data_split <- initial_split(wbcd, prop = 0.8, strata = diagnosis)
train_data <- training(data_split)
test_data <- testing(data_split)
rec <- recipe(diagnosis ~ ., data = train_data) %>%
step_normalize(all_predictors()) %>% # Normalize all predictors
prep()
train_preprocessed <- bake(rec, new_data = train_data)
test_preprocessed <- bake(rec, new_data = test_data)
knn_spec <- nearest_neighbor(neighbors = tune()) %>%  # Tune neighbors (k)
set_engine("kknn") %>%
set_mode("classification")
knn_workflow <- workflow() %>%
add_recipe(rec) %>%  # Add the preprocessing recipe
add_model(knn_spec)  # Add the model to the workflow
# Load required libraries
library(tidymodels)
library(kknn)
library(ROCR)  # For ROC curve
library(pROC)  # For AUC calculation and ROC plotting
library(ggplot2) # For visualization
library(MLmetrics) # For MCC
# Step 1: Load and Prepare Data
wbcd <- read.csv("wisc_bc_data.csv", stringsAsFactors = FALSE)
# Remove unnecessary columns (e.g., ID)
wbcd <- wbcd %>%
select(-id)
# Recode the 'diagnosis' column as a factor
wbcd <- wbcd %>%
mutate(diagnosis = factor(diagnosis, levels = c("B", "M"),
labels = c("Benign", "Malignant")))
# Step 2: Split the Data
set.seed(123) # For reproducibility
data_split <- initial_split(wbcd, prop = 0.8, strata = diagnosis)
train_data <- training(data_split)
test_data <- testing(data_split)
# Step 3: Preprocessing
# Use a recipe for consistent preprocessing steps
rec <- recipe(diagnosis ~ ., data = train_data) %>%
step_normalize(all_predictors())# Normalize all predictors
train_preprocessed <- bake(rec, new_data = train_data)
rec <- recipe(diagnosis ~ ., data = train_data) %>%
step_normalize(all_predictors())  # Normalize all predictors (numeric features)
train_preprocessed <- bake(rec, new_data = train_data)
rec <- recipe(diagnosis ~ ., data = train_data) %>%
step_normalize(all_predictors())  # Normalize all predictors (numeric features)
# Step 4: Define the Model
knn_spec <- nearest_neighbor(neighbors = tune()) %>%  # Tune neighbors (k)
set_engine("kknn") %>%
set_mode("classification")
# Step 5: Define a Workflow
knn_workflow <- workflow() %>%
add_recipe(rec) %>%  # Add the preprocessing recipe
add_model(knn_spec)  # Add
# Step 6: Cross-Validation Setup
set.seed(123)
cv_folds <- vfold_cv(train_data, v = 5, strata = diagnosis)
# Step 7: Hyperparameter Tuning
# Define grid for neighbors (k)
knn_grid <- grid_regular(neighbors(range = c(1, 31)), levels = 15)
# Tune the model
knn_results <- tune_grid(
knn_workflow,
resamples = cv_folds,
grid = knn_grid,
metrics = metric_set(accuracy)
)
# Step 8: Get the Best K (neighbors)
best_k <- knn_results %>%
select_best(metric = "accuracy")
cat("Best value of k: ", best_k$neighbors, "\n")
# Step 9: Finalize the Workflow with the Best k
final_knn <- finalize_workflow(knn_workflow, best_k)
# Step 10: Fit the Final Model
final_fit <- final_knn %>%
last_fit(data_split)
# Collect metrics from the final model
final_metrics <- collect_metrics(final_fit)
print(final_metrics)
# Make predictions on the test data
test_predictions <- predict(final_fit, new_data = test_data) %>%
bind_cols(test_data %>% select(diagnosis))
# Step 10: Make Predictions with Final Model
# ---------------------------
# Extract the fitted model and the preprocessed test data from the last_fit() result
final_model <- extract_workflow(final_fit)  # Extract the final workflow object
# Make predictions on the test data using the final fitted model
test_predictions <- predict(final_model, new_data = test_data)
# Optionally, view the predictions along with the actual labels
predictions_with_labels <- test_predictions %>%
bind_cols(test_data %>% select(diagnosis))  # Assuming 'diagnosis' is the true label
# View the first few predictions
head(predictions_with_labels)
# Confusion Matrix to assess the classification performance
conf_matrix <- test_predictions %>%
conf_mat(truth = diagnosis, estimate = .pred_class)
conf_matrix <- predictions_with_labels %>%
conf_mat(truth = diagnosis, estimate = .pred_class)
# Print the confusion matrix
print(conf_matrix)
# Step 12: MCC (Matthews Correlation Coefficient)
# Calculate MCC for the predictions
mcc_value <- MLmetrics::MCC(predictions_with_labels$.pred_class, predictions_with_labels$diagnosis)
library(yardstick)
mcc_result <- predictions_with_labels %>%
metrics(truth = diagnosis, estimate = .pred_class) %>%
filter(.metric == "mcc")
# Output the MCC value
cat("Matthews Correlation Coefficient (MCC): ", mcc_result$.estimate, "\n")
# Step
predictions_with_labels %>%
metrics(truth = diagnosis, estimate = .pred_class) %>% head()
mcc_result <- predictions_with_labels %>%
metrics(truth = diagnosis, estimate = .pred_class) %>%
dplyr::filter(.metric == "mcc")
# Output the MCC value
cat("Matthews Correlation Coefficient (MCC): ", mcc_result$.estimate, "\n")
# St
mcc_result
head(predictions_with_labels)
predictions_with_labels %>%
metrics(truth = diagnosis, estimate = .pred_class)
mcc_result <- predictions_with_labels %>%
yardstick::metrics(truth = diagnosis, estimate = .pred_class) %>%
dplyr::filter(.metric == "mcc")
# Output the MCC value
cat("Matthews Correlation Coefficient (MCC): ", mcc_result$.estimate, "\n")
# Ste
# Step 12: MCC (Matthews Correlation Coefficient)
# Calculate MCC for the predictions
# Extract values from confusion matrix
TP <- conf_matrix[2, 2]  # True Positive
conf_matrix <- predictions_with_labels %>%
conf_mat(truth = diagnosis, estimate = .pred_class)
# Print the confusion matrix
print(conf_matrix)
TP <- conf_matrix[2, 2]  # True Positive
# Step 12: MCC (Matthews Correlation Coefficient)
# Calculate MCC for the predictions
# Extract values from confusion matrix
conf_matrix <- table(predictions_with_labels$.pred_class, predictions_with_labels$diagnosis)
TP <- conf_matrix[2, 2]  # True Positive
TN <- conf_matrix[1, 1]  # True Negative
FP <- conf_matrix[1, 2]  # False Positive
FN <- conf_matrix[2, 1]  # False Negative
conf_matrix
# Compute Matthews Correlation Coefficient (MCC)
mcc_value <- (TP * TN - FP * FN) / sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))
cat("Matthews Correlation Coefficient (MCC): ", mcc_value, "\n")
# Step 13: ROC Curve
# Use pROC for ROC curve and AUC calculation
roc_curve <- roc(test_predictions$diagnosis, as.numeric(test_predictions$.pred_class) - 1)
# Output the MCC value
# Step 13: ROC Curve
# Use pROC for ROC curve and AUC calculation
roc_curve <- roc(predictions_with_labels$diagnosis, as.numeric(predictions_with_labels$.pred_class) - 1)
plot(roc_curve, main = "ROC Curve", col = "blue", lwd = 2)
# AUC Score
cat("AUC: ", auc(roc_curve), "\n")
# Step 14: Precision-Recall Curve
# For Precision-Recall curve, we use ROCR
pr_curve <- performance(prediction(as.numeric(predictions_with_labels$.pred_class) - 1,
as.numeric(predictions_with_labels$diagnosis) - 1),
"prec", "rec")
plot(pr_curve, main = "Precision-Recall Curve", col = "green", lwd = 2)
# Step 15: Visualize Precision-Recall Curve and ROC in ggplot2
roc_data <- data.frame(fpr = roc_curve$fpr, tpr = roc_curve$tpr)
ggplot(roc_data, aes(x = fpr, y = tpr)) +
geom_line(color = "blue", size = 1) +
geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
labs(x = "False Positive Rate", y = "True Positive Rate", title = "ROC Curve") +
theme_minimal()
pr_data <- data.frame(recall = pr_curve@x.values[[1]], precision = pr_curve@y.values[[1]])
ggplot(pr_data, aes(x = recall, y = precision)) +
geom_line(color = "green", size = 1) +
labs(x = "Recall", y = "Precision", title = "Precision-Recall Curve") +
theme_minimal()
# Load required libraries
library(tidymodels)
library(kknn)
library(pROC)  # For AUC calculation and ROC plotting
library(ggplot2) # For visualization
library(MLmetrics) # For MCC
# Step 1: Load and Prepare Data
wbcd <- read.csv("wisc_bc_data.csv", stringsAsFactors = FALSE)
# Remove unnecessary columns (e.g., ID)
wbcd <- wbcd %>%
select(-id)
# Recode the 'diagnosis' column as a factor
wbcd <- wbcd %>%
mutate(diagnosis = factor(diagnosis, levels = c("B", "M"),
labels = c("Benign", "Malignant")))
# Step 2: Split the Data
set.seed(123) # For reproducibility
# Split into Train+Validation (80%) and Test (20%)
train_val_split <- initial_split(wbcd, prop = 0.8, strata = diagnosis)
train_val_data <- training(train_val_split)
test_data <- testing(train_val_split)
# Now split the Train+Validation data into Train (50%) and Validation (30%)
train_split <- initial_split(train_val_data, prop = 0.5, strata = diagnosis)  # 50% for training
train_data <- training(train_split)
validation_data <- testing(train_split)  # 30% for validation (because it's 50% of 80%)
# Now split the Train+Validation data into Train (50%) and Validation (30%)
train_split <- initial_split(train_val_data, prop = 0.8, strata = diagnosis)  # 50% for training
train_data <- training(train_split)
validation_data <- testing(train_split)  # 30% for validation (because it's 50% of 80%)
# Step 3: Preprocessing
# Use a recipe for consistent preprocessing steps
rec <- recipe(diagnosis ~ ., data = train_data) %>%
step_normalize(all_predictors())  # Normalize all predictors (numeric features)
# Step 4: Define the Model
knn_spec <- nearest_neighbor(neighbors = tune()) %>%  # Tune neighbors (k)
set_engine("kknn") %>%
set_mode("classification")
# Step 5: Define a Workflow
knn_workflow <- workflow() %>%
add_recipe(rec) %>%  # Add the preprocessing recipe
add_model(knn_spec)  # Add the model to the workflow
# Step 6: Cross-Validation Setup (using validation set)
set.seed(123)
cv_folds <- vfold_cv(validation_data, v = 5, strata = diagnosis)
# Define grid for neighbors (k)
knn_grid <- grid_regular(neighbors(range = c(1, 31)), levels = 15)
# Tune the model
knn_results <- tune_grid(
knn_workflow,
resamples = cv_folds,
grid = knn_grid,
metrics = metric_set(accuracy)
)
# Step 8: Get the Best K (neighbors)
best_k <- knn_results %>%
select_best(metric = "accuracy")
cat("Best value of k: ", best_k$neighbors, "\n")
# Step 9: Finalize the Workflow with the Best k
final_knn <- finalize_workflow(knn_workflow, best_k)
# Step 10: Fit the Final Model
final_fit <- final_knn %>%
last_fit(train_val_split)  # Fit using the original train_val_split which includes 50% training and 30% validation
final_metrics <- collect_metrics(final_fit)
print(final_metrics)
# Extract the fitted model and the preprocessed test data from the last_fit() result
final_model <- extract_workflow(final_fit)  # Extract the final workflow object
# Make predictions on the test data using the final fitted model
test_predictions <- predict(final_model, new_data = test_data)
# Optionally, view the predictions along with the actual labels
predictions_with_labels <- test_predictions %>%
bind_cols(test_data %>% select(diagnosis))  # Assuming 'diagnosis' is the true label
# View the first few predictions
head(predictions_with_labels)
# Confusion Matrix to assess the classification performance
conf_matrix_yardstick <- predictions_with_labels %>%
conf_mat(truth = diagnosis, estimate = .pred_class)
# Print the confusion matrix
print(conf_matrix_yardstick)
# Calculate MCC for the predictions
manual_conf_matrix <- table(predictions_with_labels$.pred_class, predictions_with_labels$diagnosis)
TP <- manual_conf_matrix[2, 2]  # True Positive
TN <- manual_conf_matrix[1, 1]  # True Negative
FP <- manual_conf_matrix[1, 2]  # False Positive
FN <- manual_conf_matrix[2, 1]  # False Negative
# Compute Matthews Correlation Coefficient (MCC)
mcc_value <- (TP * TN - FP * FN) / sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))
cat("Matthews Correlation Coefficient (MCC): ", mcc_value, "\n")
# Step 15: ROC Curve
# ---------------------------
# Use pROC for ROC curve and AUC calculation
roc_curve <- roc(predictions_with_labels$diagnosis, as.numeric(predictions_with_labels$.pred_class) - 1)
# Plot ROC curve
plot(roc_curve, main = "ROC Curve", col = "blue", lwd = 2)
# AUC Score
cat("AUC: ", auc(roc_curve), "\n")
# For Precision-Recall curve, we use pROC
pr_curve <- roc(predictions_with_labels$diagnosis,
as.numeric(predictions_with_labels$.pred_class) - 1,
plot = TRUE, print.auc = TRUE, col = "green",
main = "Precision-Recall Curve")
# load the "gmodels" library
library(gmodels)
# Create the cross tabulation of predicted vs. actual
CrossTable(x = predictions_with_labels$diagnosis, y = predictions_with_labels$.pred_class,
prop.chisq = FALSE)
getwd()
