sms_corpus_clean <- tm_map(sms_corpus_clean, removePunctuation) # remove punctuation
as.character(sms_corpus_clean[[1]])
removePunctuation("hello...world")
replacePunctuation <- function(x) { gsub("[[:punct:]]+", " ", x) }
replacePunctuation("hello...world")
# illustration of word stemming
library(SnowballC)
wordStem(c("learn", "learned", "learning", "learns"))
sms_corpus_clean <- tm_map(sms_corpus_clean, stemDocument)
sms_corpus_clean <- tm_map(sms_corpus_clean, stripWhitespace) # eliminate unneeded whitespace
#
as.character(sms_corpus_clean[[1]])
lapply(sms_corpus[1:3], as.character)
lapply(sms_corpus_clean[1:3], as.character)
# illustration of word stemming
library(SnowballC)
wordStem(c("learn", "learned", "learning", "learns"))
# create a document-term sparse matrix
sms_dtm <- DocumentTermMatrix(sms_corpus_clean)
sms_dtm[1:5, 1:5]
# alternative solution: create a document-term sparse matrix directly from the SMS corpus
sms_dtm2 <- DocumentTermMatrix(sms_corpus, control = list(
tolower = TRUE,
removeNumbers = TRUE,
stopwords = TRUE,
removePunctuation = TRUE,
stemming = TRUE
))
# alternative solution: using custom stop words function ensures identical result
sms_dtm3 <- DocumentTermMatrix(sms_corpus, control = list(
tolower = TRUE,
removeNumbers = TRUE,
stopwords = function(x) { removeWords(x, stopwords()) },
removePunctuation = TRUE,
stemming = TRUE
))
# compare the result
sms_dtm
sms_dtm2
sms_dtm3
sms_dtm_train <- sms_dtm[1:4169, ]
sms_dtm_test  <- sms_dtm[4170:5559, ]
# also save the labels
sms_train_labels <- sms_raw[1:4169, ]$type
sms_test_labels  <- sms_raw[4170:5559, ]$type
# check that the proportion of spam is similar
prop.table(table(sms_train_labels))
prop.table(table(sms_test_labels))
# word cloud visualization
library(wordcloud)
wordcloud(sms_corpus_clean, min.freq = 50, random.order = FALSE)
# subset the training data into spam and ham groups
spam <- subset(sms_raw, type == "spam")
ham  <- subset(sms_raw, type == "ham")
wordcloud(spam$text, max.words = 40, scale = c(3, 0.5))
wordcloud(ham$text, max.words = 40, scale = c(3, 0.5))
wordcloud(spam$text, max.words = 40, scale = c(3, 0.5))
ham  <- subset(sms_raw, type == "ham")
wordcloud(ham$text, max.words = 40, scale = c(3, 0.5))
sms_dtm_freq_train <- removeSparseTerms(sms_dtm_train, 0.999)
sms_dtm_freq_train
# indicator features for frequent words
findFreqTerms(sms_dtm_train, 5)
# save frequently-appearing terms to a character vector
sms_freq_words <- findFreqTerms(sms_dtm_train, 5)
str(sms_freq_words)
# create DTMs with only the frequent terms
sms_dtm_freq_train <- sms_dtm_train[ , sms_freq_words]
sms_dtm_freq_test <- sms_dtm_test[ , sms_freq_words]
# convert counts to a factor
convert_counts <- function(x) {
x <- ifelse(x > 0, "Yes", "No")
}
# apply() convert_counts() to columns of train/test data
sms_train <- apply(sms_dtm_freq_train, MARGIN = 2, convert_counts)
sms_test  <- apply(sms_dtm_freq_test, MARGIN = 2, convert_counts)
## Step 3: Training a model on the data ----
library(e1071)
class(sms_train)
sms_train[1:5, 1:5]
sms_classifier <- naiveBayes(sms_train, sms_train_labels)
## Step 4: Evaluating model performance ----
sms_test_pred <- predict(sms_classifier, sms_test)
library(gmodels)
CrossTable(sms_test_pred, sms_test_labels,
prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
dnn = c('predicted', 'actual'))
sms_classifier2 <- naiveBayes(sms_train, sms_train_labels, laplace = 1)
sms_test_pred2 <- predict(sms_classifier2, sms_test)
CrossTable(sms_test_pred2, sms_test_labels,
prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
dnn = c('predicted', 'actual'))
# Load necessary libraries
library(tidymodels)  # For ML workflow
library(textrecipes) # For text preprocessing
library(tidyverse)   # For data manipulation and visualization
library(wordcloud)   # For word cloud visualization
# Set a seed for reproducibility
set.seed(123)
# Step 1: Load and Explore Data
sms_raw <- read_csv("sms_spam.csv")
sms_raw <- sms_raw %>%
mutate(type = as_factor(type))
# Split the data into training and testing sets
sms_split <- initial_split(sms_raw, prop = 0.75, strata = type)
sms_train <- training(sms_split)
sms_test <- testing(sms_split)
sms_recipe <- recipe(type ~ text, data = sms_train) %>%
step_tokenize(text) %>%                           # Tokenize the text column
step_stopwords(text) %>%                          # Remove stopwords
step_stem(text) %>%                               # Apply stemming
step_tokenfilter(text, max_tokens = 500) %>%      # Keep only the top 500 tokens
step_tf(text)
nb_model <- naive_Bayes() %>%
set_mode("classification") %>%
set_engine("klaR")
sms_workflow <- workflow() %>%
add_recipe(sms_recipe) %>%
add_model(nb_model)
install.packages("discrim")
library(discrim)
# Load necessary libraries
library(tidymodels)  # For ML workflow
library(discrim)     # For Naive Bayes implementation
library(textrecipes) # For text preprocessing
library(tidyverse)   # For data manipulation and visualization
library(wordcloud)   # For word cloud visualization
# Set a seed for reproducibility
set.seed(123)
# Step 1: Load and Explore Data
sms_raw <- read_csv("sms_spam.csv")
sms_raw <- sms_raw %>%
mutate(type = as_factor(type))
# Split the data into training and testing sets
sms_split <- initial_split(sms_raw, prop = 0.75, strata = type)
sms_train <- training(sms_split)
sms_test <- testing(sms_split)
# Step 2: Data Preprocessing
sms_recipe <- recipe(type ~ text, data = sms_train) %>%
step_tokenize(text) %>%                           # Tokenize the text column
step_stopwords(text) %>%                          # Remove stopwords
step_stem(text) %>%                               # Apply stemming
step_tokenfilter(text, max_tokens = 500) %>%      # Keep only the top 500 tokens
step_tf(text)                                     # Convert to term frequency
# Step 3: Define the Model
nb_model <- naive_Bayes() %>%
set_mode("classification") %>%
set_engine("klaR")  # Supported by discrim
sms_workflow <- workflow() %>%
add_recipe(sms_recipe) %>%
add_model(nb_model)
sms_fit <- sms_workflow %>%
fit(data = sms_train)
install.packages("stopwords")
library(stopwords)
# Step 2: Data Preprocessing
sms_recipe <- recipe(type ~ text, data = sms_train) %>%
step_tokenize(text) %>%                           # Tokenize the text column
step_stopwords(text) %>%                          # Remove stopwords
step_stem(text) %>%                               # Apply stemming
step_tokenfilter(text, max_tokens = 500) %>%      # Keep only the top 500 tokens
step_tf(text)                                     # Convert to term frequency
# Step 3: Define the Model
nb_model <- naive_Bayes() %>%
set_mode("classification") %>%
set_engine("klaR")  # Supported by discrim
# Step 4: Create a Workflow
sms_workflow <- workflow() %>%
add_recipe(sms_recipe) %>%
add_model(nb_model)
# Step 5: Train the Model
sms_fit <- sms_workflow %>%
fit(data = sms_train)
install.packages("klaR")
library(klaR)
# Step 5: Train the Model
sms_fit <- sms_workflow %>%
fit(data = sms_train)
sms_predictions <- sms_fit %>%
predict(new_data = sms_test) %>%
bind_cols(sms_test)
sms_metrics <- sms_predictions %>%
metrics(truth = type, estimate = .pred_class)
print(sms_metrics)
# Step 7: Visualize Word Frequencies
sms_train %>%
filter(type == "spam") %>%
unnest_tokens(word, text) %>%
count(word, sort = TRUE) %>%
with(wordcloud(word, n, max.words = 40, scale = c(3, 0.5)))
install.packages("tidytext")
library(tidytext)
# Step 7: Visualize Word Frequencies
sms_train %>%
filter(type == "spam") %>%
unnest_tokens(word, text) %>%
count(word, sort = TRUE) %>%
with(wordcloud(word, n, max.words = 40, scale = c(3, 0.5)))
sms_train %>%
filter(type == "ham") %>%
unnest_tokens(word, text) %>%
count(word, sort = TRUE) %>%
with(wordcloud(word, n, max.words = 40, scale = c(3, 0.5)))
nb_model_laplace <- naive_Bayes(smoothness = 1) %>%
set_mode("classification") %>%
set_engine("klaR")
sms_workflow_laplace <- sms_workflow %>%
update_model(nb_model_laplace)
sms_fit_laplace <- sms_workflow_laplace %>%
fit(data = sms_train)
sms_predictions_laplace <- sms_fit_laplace %>%
predict(new_data = sms_test) %>%
bind_cols(sms_test)
sms_metrics_laplace <- sms_predictions_laplace %>%
metrics(truth = type, estimate = .pred_class)
print(sms_metrics_laplace)
head(sms_metrics)
sms_metrics$.estimate
sms_metrics
sms_predictions
# load the "gmodels" library
library(gmodels)
# Create the cross tabulation of predicted vs. actual
CrossTable(
x = sms_predictions$type,
y = sms_predictions$.pred_class,
prop.chisq = FALSE
)
# Create the cross tabulation of predicted vs. actual
CrossTable(
y = sms_predictions$type,
x = sms_predictions$.pred_class,
prop.chisq = FALSE
)
# Create the cross tabulation of predicted vs. actual
CrossTable(sms_predictions$.pred_class, ms_predictions$type,
prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
dnn = c('predicted', 'actual')
)
# Create the cross tabulation of predicted vs. actual
CrossTable(sms_predictions$.pred_class, sms_predictions$type,
prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
dnn = c('predicted', 'actual')
)
# Path to the scripts
library("rstudioapi")
# Set working directory
setwd(dirname(getActiveDocumentContext()$path))  # Set the working directory to the parent directory of the active document
# Load required libraries
library(tidymodels)
library(discrim)     # For Naive Bayes
library(textrecipes) # For text preprocessing (if needed for text data)
library(pROC)        # For AUC and ROC
library(MLmetrics)   # For MCC
library(gmodels)     # For detailed confusion matrices
library(ggplot2)     # For visualization
# Set a seed for reproducibility
set.seed(123)
# Step 1: Load and Prepare Data
wbcd <- read.csv("wisc_bc_data.csv", stringsAsFactors = FALSE)
getwd()
# Set a seed for reproducibility
set.seed(123)
# Step 1: Load and Prepare Data
wbcd <- read.csv("wisc_bc_data.csv", stringsAsFactors = FALSE)
# Remove unnecessary columns (e.g., ID)
wbcd <- wbcd %>%
select(-id)
# Step 1: Load and Prepare Data
wbcd <- read.csv("wisc_bc_data.csv", stringsAsFactors = FALSE)
wbcd <- wbcd %>%
dplyr::select(-id)
# Recode the 'diagnosis' column as a factor
wbcd <- wbcd %>%
mutate(diagnosis = factor(
diagnosis,
levels = c("B", "M"),
labels = c("Benign", "Malignant")
))
table(wbcd$diagnosis)
# Split into Train+Validation (80%) and Test (20%)
train_val_split <- initial_split(wbcd, prop = 0.8, strata = diagnosis)
train_val_data <- training(train_val_split)
test_data <- testing(train_val_split)
# Now split the Train+Validation data into Train (50%) and Validation (30%)
train_split <- initial_split(train_val_data, prop = 0.8, strata = diagnosis)  # 50% for training
train_data <- training(train_split)
validation_data <- testing(train_split)  # 30% for validation (because it's 50% of 80%)
# Step 3: Preprocessing
# Use a recipe for consistent preprocessing steps
rec <- recipe(diagnosis ~ ., data = train_data) %>%
step_normalize(all_predictors())  # Normalize all predictors (numeric features)
# Step 4: Define the Naive Bayes Model
nb_spec <- naive_Bayes() %>%
set_mode("classification") %>%
set_engine("klaR")  # Naive Bayes engine from `klaR` package
# Step 5: Define a Workflow
nb_workflow <- workflow() %>%
add_recipe(rec) %>%  # Add the preprocessing recipe
add_model(nb_spec)   # Add the Naive Bayes model to the workflow
# Step 6: Cross-Validation Setup
set.seed(123)
cv_folds <- vfold_cv(validation_data, v = 10, strata = diagnosis)
# Step 7: Train and Evaluate Model using Cross-Validation
nb_results <- fit_resamples(
nb_workflow,
resamples = cv_folds,
metrics = metric_set(accuracy, roc_auc)
)
nb_metrics <- collect_metrics(nb_results)
print(nb_metrics)
# Step 8: Fit the Final Model
final_nb_fit <- nb_workflow %>%
last_fit(train_val_split)  # Fit using the original train_val_split (50% training, 30% validation)
# Step 9: Evaluate the Final Model
final_nb_metrics <- collect_metrics(final_nb_fit)
print(final_nb_metrics)
# Step 10: Make Predictions with Final Model
final_model <- extract_workflow(final_nb_fit)  # Extract the final workflow object
test_predictions <- predict(final_model, new_data = test_data) %>%
bind_cols(test_data %>% select(diagnosis))  # Assuming 'diagnosis' is the true label
test_predictions <- predict(final_model, new_data = test_data) %>%
bind_cols(test_data %>% dplyr::select(diagnosis))  # Assuming 'diagnosis' is the true label
# View predictions
head(test_predictions)
# Step 11: Confusion Matrix
conf_matrix <- test_predictions %>%
conf_mat(truth = diagnosis, estimate = .pred_class)
print(conf_matrix)
# Step 12: MCC (Matthews Correlation Coefficient)
manual_conf_matrix <- table(test_predictions$.pred_class, test_predictions$diagnosis)
TP <- manual_conf_matrix[2, 2]  # True Positive
TN <- manual_conf_matrix[1, 1]  # True Negative
FP <- manual_conf_matrix[1, 2]  # False Positive
FN <- manual_conf_matrix[2, 1]  # False Negative
mcc_value <- (TP * TN - FP * FN) / sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))
cat("Matthews Correlation Coefficient (MCC): ", mcc_value, "\n")
# Step 13: ROC Curve
roc_curve <- roc(
test_predictions$diagnosis,
as.numeric(test_predictions$.pred_class) - 1
)
plot(roc_curve,
main = "ROC Curve",
col = "blue",
lwd = 2)
cat("AUC: ", auc(roc_curve), "\n")
# Step 14: Precision-Recall Curve
pr_curve <- roc(
test_predictions$diagnosis,
as.numeric(test_predictions$.pred_class) - 1,
plot = TRUE,
print.auc = TRUE,
col = "green",
main = "Precision-Recall Curve"
)
# Step 15: Cross Table
CrossTable(
x = test_predictions$diagnosis,
y = test_predictions$.pred_class,
prop.chisq = FALSE
)
# Path to the scripts
library("rstudioapi")
# Set working directory
setwd(dirname(getActiveDocumentContext()$path))  # Set the working directory to the parent directory of the active document
# Load required libraries
library(tidymodels)
library(naivebayes)
library(pROC)  # For AUC calculation and ROC plotting
library(ggplot2) # For visualization
library(MLmetrics) # For MCC
library(gmodels)  # For cross-tabulation
library(dplyr) # For dplyr functions
# Step 1: Load and Prepare Data
wbcd <- read.csv("wisc_bc_data.csv", stringsAsFactors = FALSE)
# Remove unnecessary columns (e.g., ID)
wbcd <- wbcd %>%
dplyr::select(-id)
# Recode the 'diagnosis' column as a factor
wbcd <- wbcd %>%
dplyr::mutate(diagnosis = factor(
diagnosis,
levels = c("B", "M"),
labels = c("Benign", "Malignant")
))
# Display counts of each diagnosis type
table(wbcd$diagnosis)
# Step 2: Split the Data
set.seed(123) # For reproducibility
# Split into Train+Validation (80%) and Test (20%)
train_val_split <- initial_split(wbcd, prop = 0.8, strata = diagnosis)
train_val_data <- training(train_val_split)
test_data <- testing(train_val_split)
# Now split the Train+Validation data into Train (50%) and Validation (30%)
train_split <- initial_split(train_val_data, prop = 0.8, strata = diagnosis)  # 50% for training
train_data <- training(train_split)
validation_data <- testing(train_split)  # 30% for validation (because it's 50% of 80%)
# Step 3: Preprocessing with recipe
# Use a recipe for consistent preprocessing steps
rec <- recipe(diagnosis ~ ., data = train_data) %>%
step_normalize(all_predictors())  # Normalize all predictors (numeric features)
# Step 4: Define the Naive Bayes Model Specification
# Define Naive Bayes model with a tuning parameter for Laplace smoothing
nb_spec <- naive_Bayes(smoothness = tune()) %>%
set_engine("naivebayes") %>%
set_mode("classification")
# Step 5: Define a Workflow
# Create a workflow that combines the recipe and model specification
nb_workflow <- workflow() %>%
add_recipe(rec) %>%  # Add preprocessing recipe
add_model(nb_spec)  # Add Naive Bayes model
# Step 6: Cross-Validation Setup (using validation set)
set.seed(123)
cv_folds <- vfold_cv(validation_data, v = 10, strata = diagnosis)
# Step 7: Hyperparameter Tuning for Laplace smoothing
# Define a grid for the smoothness (Laplace) parameter
laplace_grid <- grid_regular(
smoothness(range = c(0.5, 3)),  # Tune Laplace smoothing factor between 0.5 and 3
levels = 10
)
# Tune the model using the grid and cross-validation
nb_tune_results <- tune_grid(
nb_workflow,
resamples = cv_folds,
grid = laplace_grid,
metrics = metric_set(accuracy)
)
# Step 8: Get the Best Laplace Parameter
best_nb_tune <- nb_tune_results %>%
dplyr::select_best("accuracy")
best_nb_tune <- nb_tune_results %>%
select_best("accuracy")
head(nb_tune_results)
# Step 8: Get the Best Laplace Parameter
best_nb_tune <- nb_tune_results %>%
dplyr::select_best("accuracy")
# Step 8: Get the Best Laplace Parameter
best_nb_tune <- nb_tune_results %>%
tune::select_best("accuracy")
# Step 8: Get the Best Laplace Parameter
best_nb_tune <- nb_tune_results %>%
tune::select_best("accuracy")
# Step 8: Get the Best Laplace Parameter
best_nb_tune <- tune::select_best(nb_tune_results, metric = "accuracy")
cat("Best Laplace smoothing value: ", best_nb_tune$smoothness, "\n")
# Step 9: Finalize the Workflow with the Best Laplace Parameter
final_nb_workflow <- finalize_workflow(nb_workflow, best_nb_tune)
# Step 10: Fit the Final Model
final_nb_fit <- final_nb_workflow %>%
last_fit(train_val_split)  # Fit the model on the train_val_split
# Step 11: Evaluate the Model
# Collect and view final performance metrics
final_metrics <- collect_metrics(final_nb_fit)
print(final_metrics)
# Extract the fitted model and the preprocessed test data from the last_fit() result
final_model <- extract_workflow(final_nb_fit)  # Extract the final workflow object
# Make predictions on the test data using the final fitted model
test_predictions <- predict(final_model, new_data = test_data)
# Optionally, view the predictions along with the actual labels
predictions_with_labels <- test_predictions %>%
dplyr::bind_cols(test_data %>% dplyr::select(diagnosis))  # Assuming 'diagnosis' is the true label
# View the first few predictions
head(predictions_with_labels)
# Confusion Matrix to assess the classification performance
conf_matrix_yardstick <- predictions_with_labels %>%
conf_mat(truth = diagnosis, estimate = .pred_class)
# Print the confusion matrix
print(conf_matrix_yardstick)
# Step 14: MCC (Matthews Correlation Coefficient)
# ---------------------------
# Calculate MCC for the predictions
manual_conf_matrix <- table(predictions_with_labels$.pred_class,
predictions_with_labels$diagnosis)
TP <- manual_conf_matrix[2, 2]  # True Positive
TN <- manual_conf_matrix[1, 1]  # True Negative
FP <- manual_conf_matrix[1, 2]  # False Positive
FN <- manual_conf_matrix[2, 1]  # False Negative
# Compute Matthews Correlation Coefficient (MCC)
mcc_value <- (TP * TN - FP * FN) / sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))
cat("Matthews Correlation Coefficient (MCC): ", mcc_value, "\n")
# Step 15: ROC Curve
# ---------------------------
# Use pROC for ROC curve and AUC calculation
roc_curve <- roc(
predictions_with_labels$diagnosis,
as.numeric(predictions_with_labels$.pred_class) - 1
)
# Plot ROC curve
plot(roc_curve,
main = "ROC Curve",
col = "blue",
lwd = 2)
# AUC Score
cat("AUC: ", auc(roc_curve), "\n")
# Step 16: Precision-Recall Curve
# ---------------------------
# For Precision-Recall curve, we use pROC
pr_curve <- roc(
predictions_with_labels$diagnosis,
as.numeric(predictions_with_labels$.pred_class) - 1,
plot = TRUE,
print.auc = TRUE,
col = "green",
main = "Precision-Recall Curve"
)
# Create the cross tabulation of predicted vs. actual
CrossTable(
x = predictions_with_labels$diagnosis,
y = predictions_with_labels$.pred_class,
prop.chisq = FALSE
)
